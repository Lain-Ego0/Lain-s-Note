# 1.关于强化学习

强化学习(RL)是一种面向决策与控制的计算方法，主要研究智能体如何在复杂、不确定的环境中，通过与环境的交互来学习最优策略，以实现长期累积奖励（Cumulative Reward）的最大化。与监督学习依赖预先标记的数据集不同，强化学习不提供正确答案作为标签，而是通过环境反馈的**标量奖励信号**进行试错学习。部分专业术语因中文表达不够精准，将直接使用英文表述。

## （一）强化学习简介

### 1. 核心组件与数学模型

强化学习问题通常被数学化描述为 **马尔可夫决策过程 (MDP)**，该过程由一个五元组 $(S, A, P, R, \gamma)$ 定义：

- **状态空间 (S, State Space)**：环境在特定时刻的客观描述。对于机器人系统，状态可能包含关节角度、角速度、IMU数据等传感器读数；对于游戏，状态即为当前的像素帧或内存数据。
- **动作空间 (A, Action Space)**：智能体在给定状态下可执行的所有操作集合。动作可以是离散的（如“左移”“右移”）或连续的（如电机输出的电压值或力矩）。
- **状态转移概率 (P, Transition Probability)**：描述环境动力学的模型。即在状态 $S_t$ 下执行动作 $A_t$ 后，转移到状态 $S_{t+1}$ 的概率分布。在无模型 (Model-Free) RL 中，此转移概率通常是未知的。
- **奖励函数 (R, Reward Function)**：环境向智能体提供的即时反馈信号 $R_t$。通常定义为一个标量数值，用于评估在状态 $S_t$ 采取动作 $A_t$ 的好坏。
- **折扣因子 ($\gamma$, Discount Factor)**：取值范围通常在 $[0, 1]$ 之间，用于衡量未来奖励对当前决策的重要性。$\gamma$ 越接近 1，表示智能体越重视长期收益。

### 2. 运行机制与控制回路

强化学习的执行过程是一个离散的时间步循环，核心流程如下：

1. **观测**：在时刻 $t$，智能体观测到当前环境状态 $S_t$。
2. **决策**：智能体根据当前的策略 $\pi$，选择一个动作 $A_t = \pi(S_t)$。策略本质上是一个从状态到动作的映射函数。
3. **执行与反馈**：智能体在环境中执行动作 $A_t$。环境根据动力学规律转移到新的状态 $S_{t+1}$，并根据奖励函数计算出即时奖励 $R_t$。
4. **学习与更新**：智能体接收 $(S_t, A_t, R_t, S_{t+1})$ 这一组经验数据，利用特定的算法更新其策略网络或价值函数，以优化未来的决策能力。

### 3. 优化目标：价值函数

强化学习的根本目标是寻找一个**最优策略** $\pi^*$，使得从初始状态出发所能获得的**期望回报**最大化。为了评估策略的优劣，引入**价值函数**：

- **状态价值函数 $V(s)$**：衡量在状态 $s$ 下，按照当前策略行动所能获得的期望累积奖励。
- **动作价值函数 $Q(s, a)$**：衡量在状态 $s$ 下，先采取动作 $a$，随后按照当前策略行动所能获得的期望累积奖励。

训练过程即是通过不断迭代，使得预测的价值函数收敛，并据此调整策略分布，使其向高价值动作偏移。

## （二）常用算法解析

### 1. 关于PPO 与 DQN

两者分别代表了强化学习中的两大核心流派：**策略梯度 (Policy Gradient)** 与 **价值学习 (Value-based)**

| 维度           | DQN (Deep Q-Network)                                                                 | PPO (Proximal Policy Optimization)                                                   |
|----------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| 核心机制       | 基于价值 (Value-based)，拟合 Q 函数，通过 $Q(s,a)$ 推导最优动作                        | 基于策略 (Policy-based / Actor-Critic)，直接拟合策略 $\pi(a|s)$，通过策略梯度优化参数 |
| 动作空间       | 离散 (Discrete)，仅适用于有限个离散动作（如按键）；处理连续动作需离散化，精度受限       | 连续 (Continuous) & 离散，原生支持高维连续动作空间（如机器人关节力矩），输出概率分布   |
| 采样策略       | Off-Policy (异策略)，使用经验回放池 (Replay Buffer)，可利用历史数据，样本利用率高       | On-Policy (同策略)，仅能使用当前策略采样的数据进行更新，更新后数据作废，样本利用率较低 |
| 收敛稳定性     | 较低，Q 值高估 (Overestimation) 风险较高，对超参数敏感，训练曲线波动大                  | 较高，限制策略更新幅度，训练曲线通常单调上升，不易崩溃                                |
| 并行化能力     | 较难，依赖中心化的 Replay Buffer，分布式训练通信开销大                                  | 容易，多个 Worker 并行采样，收集完一波数据统一更新，适合多核 CPU/GPU 加速              |

### 2. 为何 PPO 是当前的主流选择

尽管 PPO 在样本利用率 上不如 SAC、TD3 等 Off-policy 算法，但它凭借**稳定性**、**通用性**和**调参友好度**，成为 OpenAI 等机构的首选基准算法，尤其在机器人控制和大型语言模型领域应用广泛。

#### 2.1 核心优势：数值稳定性与截断机制 (Clipping)

策略梯度方法（如早期的 Vanilla Policy Gradient）存在严重缺陷：若一次更新步长过大，策略可能发生剧烈变化，导致性能崩塌且难以恢复。PPO 通过引入**截断目标函数 **解决此问题，核心公式如下：

$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) ]
$$
其中，$r_t(\theta)$ 是新旧策略的概率比，$\epsilon$ 是超参数（通常取 0.2）。

- **机制**：当新策略与旧策略差异过大（超过 $1 \pm \epsilon$ 范围）时，目标函数会被截断。
- **效果**：强制限制策略更新幅度，形成类似 TRPO 的信任区域，但无需计算复杂的共轭梯度或海森矩阵 (Hessian Matrix)，极大降低计算成本并保证性能单调改进。

#### 2.2 原生支持连续控制

在机器人运动控制、自动驾驶等物理仿真场景中，动作通常是连续数值。PPO 的 Actor 网络可直接输出高斯分布的均值 ($\mu$) 和标准差 ($\sigma$)，从而直接采样连续动作，天然契合物理控制任务。

#### 2.3 调参友好与工程实现便捷性

相比于 DDPG 或 AC 算法，PPO 对学习率、初始化权重等超参数不敏感，默认参数即可在多种任务中获得良好表现 。同时PPO 仅需一阶梯度优化，易于在 PyTorch 或 TensorFlow 中实现，且便于扩展到大规模并行环境（如 Isaac Gym）。