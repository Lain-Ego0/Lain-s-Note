# 1.四足强化学习扫盲

# （一）什么是强化学习

强化学习(RL)是一种面向决策与控制的计算方法，主要研究智能体如何在复杂、不确定的环境中，通过与环境的交互来学习最优策略，以实现长期累积奖励（Cumulative Reward）的最大化。与监督学习依赖预先标记的数据集不同，强化学习不提供正确答案作为标签，而是通过环境反馈的**标量奖励信号**进行试错学习。

强化学习问题通常被数学化描述为 **马尔可夫决策过程 (MDP)**，该过程由一个五元组 $(S, A, P, R, \gamma)$ 定义：

- **状态空间 (S, State Space)**：环境在特定时刻的客观描述。对于机器人系统，状态可能包含关节角度、角速度、IMU数据等传感器读数；对于游戏，状态即为当前的像素帧或内存数据。
- **动作空间 (A, Action Space)**：智能体在给定状态下可执行的所有操作集合。动作可以是离散的或连续的。
- **状态转移概率 (P, Transition Probability)**：描述环境动力学的模型。即在状态 $S_t$ 下执行动作 $A_t$ 后，转移到状态 $S_{t+1}$ 的概率分布。在无模型 (Model-Free) RL 中，此转移概率通常是未知的。
- **奖励函数 (R, Reward Function)**：环境向智能体提供的即时反馈信号 $R_t$。通常定义为一个标量数值，用于评估在状态 $S_t$ 采取动作 $A_t$ 的好坏。
- **折扣因子 ($\gamma$, Discount Factor)**：取值范围通常在 $[0, 1]$ 之间，用于衡量未来奖励对当前决策的重要性。$\gamma$ 越接近 1，表示智能体越重视长期收益。

强化学习的执行过程是一个离散的时间步循环，核心流程如下：

1. **观测**：在时刻 $t$，智能体观测到当前环境状态 $S_t$。
2. **决策**：智能体根据当前的策略 $\pi$，选择一个动作 $A_t = \pi(S_t)$。策略本质上是一个从状态到动作的映射函数。
3. **执行与反馈**：智能体在环境中执行动作 $A_t$。环境根据动力学规律转移到新的状态 $S_{t+1}$，并根据奖励函数计算出即时奖励 $R_t$。
4. **学习与更新**：智能体接收 $(S_t, A_t, R_t, S_{t+1})$ 这一组经验数据，利用特定的算法更新其策略网络或价值函数，以优化未来的决策能力。

强化学习的根本目标是寻找一个**最优策略** $\pi^*$，使得从初始状态出发所能获得的**期望回报**最大化。为了评估策略的优劣，引入**价值函数**：

- **状态价值函数 $V(s)$**：衡量在状态 $s$ 下，按照当前策略行动所能获得的期望累积奖励。
- **动作价值函数 $Q(s, a)$**：衡量在状态 $s$ 下，先采取动作 $a$，随后按照当前策略行动所能获得的期望累积奖励。

训练过程即是通过不断迭代，使得预测的价值函数收敛，并据此调整策略分布，使其向高价值动作偏移。

# （二）什么是PPO？

https://arxiv.org/abs/1707.06347

算法理解可看：https://zhuanlan.zhihu.com/p/614115887

近端策略优化（PPO）算法是OpenAI在2017提出的一种强化学习算法，被认为是目前强化学习领域的SOTA方法，也是适用性最广的算法之一。尽管 PPO 在样本利用率 上不如 SAC、TD3 等 Off-policy 算法，但它凭借**稳定性**、**通用性**和**调参友好度**，成为 OpenAI 等机构的首选基准算法，尤其在机器人控制和大型语言模型领域应用广泛。

策略梯度方法（如早期的 Vanilla Policy Gradient）存在严重缺陷：若一次更新步长过大，策略可能发生剧烈变化，导致性能崩塌且难以恢复。PPO 通过引入**截断目标函数 **解决此问题，核心公式如下：

$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) ]
$$
其中，$r_t(\theta)$ 是新旧策略的概率比，$\epsilon$ 是超参数（通常取 0.2）。

- **机制**：当新策略与旧策略差异过大（超过 $1 \pm \epsilon$ 范围）时，目标函数会被截断。
- **效果**：强制限制策略更新幅度，形成类似 TRPO 的信任区域，但无需计算复杂的共轭梯度或海森矩阵 (Hessian Matrix)，极大降低计算成本并保证性能单调改进。

在机器人运动控制、自动驾驶等物理仿真场景中，动作通常是连续数值。PPO 的 Actor 网络可直接输出高斯分布的均值 ($\mu$) 和标准差 ($\sigma$)，从而直接采样连续动作，更契合物理控制任务。相比于 DDPG 或 AC 算法，PPO 对学习率、初始化权重等超参数不敏感，默认参数即可在多种任务中获得良好表现 。同时PPO 仅需一阶梯度优化，易于在 PyTorch 或 TensorFlow 中实现，且便于扩展到大规模并行环境。

# （三）什么是isaacgym、isaacsim、isaaclab？

**Isaac Gym**是NVIDIA推出的基于PhysX、主打GPU高性能的机器人学习物理模拟器，虽能实现高帧率物理模拟并应用于多类机器人研究但功能不通用。

随后NVIDIA推出构建在Omniverse上的通用机器人模拟器**Isaac Sim**，整合了Isaac Gym的功能，还补充了高保真渲染、ROS/ROS2支持、传感器模拟等通用能力，同时配套有基于Isaac Gym的IsaacGymEnvs和基于Isaac Sim的OmniIsaacGymEnvs这两个开源环境集，不过这些环境集仅用于展示模拟器功能和基准测试，无法满足定制环境与算法的开发需求。

在此基础上，NVIDIA进一步推出建立在Isaac Sim之上的**Isaac Lab**，它是开源、模块化且可扩展的统一机器人学习框架，不仅继承了Isaac Sim的功能，还新增执行器动力学、程序地形生成等机器人学习相关功能，专门用于简化定制环境和算法的开发测试，同时取代了此前的框架，成为Isaac Sim生态下单一的机器人学习框架。

# （四）什么是legged_gym

https://github.com/leggedrobotics/legged_gym

legged_gym是一个非常经典的基于 **NVIDIA Isaac Gym** 的足式机器人强化学习代码库，由 ETH Zurich-RSL开发。通常与rsl_rl配合使用。

## 1. 核心目录结构 (Directory Structure)
### 1.1 根目录

```text
legged_gym/
├── .gitattributes
├── .gitignore
├── LICENSE                 # MIT 许可证文件
├── README.md               # 项目说明文档
├── setup.py                # 安装脚本 (pip install -e .)
├── licenses/               # 第三方依赖的许可证说明
├── resources/              # [资源库] 存放机器人模型和资产
│   └── robots/             # 具体的机器人文件
│       ├── a1/             # Unitree A1 的 URDF/Meshes
│       ├── anymal_c/       # ANYmal C 的 URDF/Meshes
│       ├── cassie/         # Cassie 的 URDF/Meshes
│       └── ...
└── legged_gym/             # [核心源码] Python 源代码包
    ├── ... 
```

-----

### 1.2 核心源码目录 (`legged_gym/legged_gym/`)

这是所有逻辑代码所在的地方，分为 `envs`（环境）、`scripts`（脚本）、`utils`（工具）三大块。

#### 1.2.1 `envs/` - 环境与物理仿真 

这里定义了机器人的物理行为、奖励函数和观察空间。

```text
legged_gym/envs/
├── __init__.py             # 任务注册表 (Task Registry) 的入口
├── base/                   # [基类] 所有足式机器人的父类
│   ├── __init__.py
│   ├── legged_robot.py     # 核心类 LeggedRobot。处理 step(), compute_reward(), reset()
│   └── legged_robot_config.py # 核心配置类 LeggedRobotCfg。定义所有超参数基类
├── a1/                     # [示例] Unitree A1 机器人
│   ├── __init__.py
│   └── a1_config.py        # 继承自 LeggedRobotCfg，定义 A1 特有的物理参数和奖励权重
├── anymal_c/               # [示例] ANYmal C 机器人
│   ├── __init__.py
│   ├── anymal_c_config.py  # ANYmal C 的配置文件
│   ├── anymal_c_rough_config.py # 针对崎岖地形的特定配置
│   └── flat/               # (可选) 平地任务的特定配置
├── cassie/                 # [示例] Cassie 机器人
│   ├── __init__.py
│   └── cassie_config.py
└── ... (其他机器人)
```

#### 1.2.2 `scripts/` - 运行入口

用户直接交互的脚本文件。

```text
legged_gym/scripts/
├── __init__.py
├── train.py                # [训练] 主入口。解析参数 -> 建立环境 -> 启动 PPO 训练
├── play.py                 # [推理] 主入口。加载 .pt 模型 -> 渲染环境 -> 键盘控制机器人
└── test.py                 
```

#### 1.2.3 `utils/` - 通用工具库

为环境提供数学计算、地形生成和日志支持。

```text
legged_gym/utils/
├── __init__.py
├── terrain.py              # [地形] 核心地形生成算法 (楼梯、斜坡、柏林噪声地形等)
├── math.py                 # [数学] 四元数运算、角度标准化、张量处理工具
├── helpers.py              # [辅助] 命令行参数解析、类加载器
├── logger.py               # [日志] 处理 Tensorboard 数据记录和图表绘制
└── task_registry.py        # [注册] 管理环境名称到类(Env, Config)的映射
```

#### 1.2.4 如何快速上手这份代码？

1.  **想改奖励函数？**
      * 直接看 `envs/base/legged_robot.py` 中的 `_reward_...` 函数，或者在具体的 `xxx_config.py` 中调整 `rewards` 类的权重。
2.  **想换个机器人？**
      * 在 `resources/` 放好 URDF。
      * 在 `envs/` 下复制一个文件夹（如 `a1`），修改 `config.py` 中的 `asset.file` 路径。
3.  **想改地形难度？**
      * 看 `utils/terrain.py` 里的生成逻辑，或者在 `config.py` 里调整 `terrain` 类的参数。


## 2. 关键模块详细解析

#### 2.1 环境基石 (`envs/base`)

这是框架中最重要的一层，采用了**面向对象**的设计，方便扩展新机器人。

1. **`legged_robot.py` (类 `LeggedRobot`)**:
   - 继承自 `BaseTask` (Isaac Gym 的接口)。
   - **`step()`**: 物理仿真的步进函数。执行动作 -> 物理引擎计算 -> 获取观察值 -> 计算奖励 -> 检查重置条件。
   - **`_init_buffers()`**: 初始化 GPU 张量（状态、观察、奖励等），利用 Isaac Gym 的并行特性，所有环境的数据都在同一个大张量中。
   - **`_reward_<name>()`**: 定义具体的奖励函数（如：跟踪速度、惩罚关节扭矩、惩罚碰撞等）。代码会自动扫描所有以 `_reward_` 开头的方法并根据配置权重求和。
   - **`_get_noise_scale()`**: 为观察值添加噪声，这是 Sim-to-Real（仿真到真机）成功的关键。
2. **`legged_robot_config.py` (类 `LeggedRobotCfg`)**:
   - 这是一个纯配置类，包含了所有超参数。
   - **`env`**: 观察空间维度、动作空间维度、环境数量。
   - **`terrain`**: 地形类型（平地、楼梯）、网格尺寸。
   - **`commands`**: 机器人接收的指令范围（线速度 x/y, 角速度 yaw）。
   - **`init_state`**: 机器人初始姿态。
   - **`control`**: PD 控制器的 P 和 D 增益、控制频率。
   - **`rewards`**: 各种奖励项的权重。

#### 2.2 具体机器人实现 (例如 `envs/a1/`)

当你需要添加一个新机器人（如 Unitree A1）时，通常只需要继承基类并覆盖少量配置：

- **继承**: `class A1(LeggedRobot)` 和 `class A1RoughCfg(LeggedRobotCfg)`。
- **重写**: 指定 URDF 路径、修改 PD 参数、调整特定的奖励权重或物理属性。

#### 2.3 地形生成 (`utils/terrain.py`)

- 实现了基于 **Curriculum Learning (课程学习)** 的地形系统。
- 机器人一开始在简单的地形训练，随着性能提升（生存时间变长、奖励变高），会被自动“晋升”到更难的地形（更高的台阶、更陡的斜坡）。
- 生成主要通过高度图 (Heightfield) 或三角网格 (Trimesh) 实现。

#### 2.4 任务注册 (`envs/__init__.py`)

- 使用 `task_registry` 单例来管理环境。
- 通过 `task_registry.register("任务名", EnvClass, EnvConfig, TrainConfig)` 将字符串名字映射到具体的类。

## 3. 数据流与控制流 (Workflow)

#### 训练流程 (`train.py`)

1. **参数解析**: 读取命令行参数 (如 `--task=a1_flat`)。
2. **环境构建**: `task_registry.make_env(name, args)`
   - 实例化 `LeggedRobot`。
   - 加载 Isaac Gym 物理引擎，创建数千个并行的机器人实例。
3. **算法初始化**: `task_registry.make_alg_runner(env, name, args)`
   - 通常实例化 `rsl_rl` 库中的 `OnPolicyRunner` (PPO 算法)。
4. **循环**:
   - `env.step(actions)` -> 返回 `obs, rewards, dones`。
   - PPO 收集数据 -> 更新神经网络策略。
   - 保存模型 (`logs/` 目录下)。

#### 推理流程 (`play.py`)

1. 加载训练好的 `model_*.pt` 权重。

2. 创建环境（通常 `num_envs` 设置较小，并开启渲染 `headless=False`）。

3. 运行策略网络输出动作，驱动机器人运动，并通过键盘/手柄实时改变指令（速度命令）。

   
